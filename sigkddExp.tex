\documentclass[sigconf,anonymous]{acmart}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

% \bibliographystyle{acm}
% \bibliography{citation.bib} 
%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}

\usepackage{color}
\usepackage{ulem}

% \usepackage{natbib} 
% \addbibresource{citation.bib}
% \printbibliography
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

\begin{document}
\bibliographystyle{ACM-Reference-Format}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Incremental Learning On Various Feature Space}

% 
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Hanmo Liu}
\affiliation{%
  \institution{Hong Kong University of Science and Technology}
  \city{Hong Kong SAR}
  \country{China}}
\email{hliubm@connect.ust.hk}

\author{Shimin Di}
\affiliation{%
  \institution{Hong Kong University of Science and Technology}
  \city{Hong Kong SAR}
  \country{China}}
\email{sdiaa@connect.ust.hk}

\author{Lei Chen}
\affiliation{%
 \institution{Hong Kong University of Science and Technology}
 \city{Hong Kong SAR}
 \country{China}}
\email{leichen@cse.ust.hk}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Liu and Di, et al.}

\begin{abstract}

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
\end{CCSXML}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{incremental learning, neural networks}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Incremental Learning, which is also called Continual Learning or Lifelong Learning, is aiming at the problem of training a model that can sequentially learn different tasks. However, there is a great obstacle in the way of achieving this goal. Because currently the deep learning models are data-driven, when given the condition that only new task's data is accessible and old tasks' data missing, the models would lose the memory of old tasks easily. This hardship is known as \textit{Catastrophic Forgetting} \cite{french1999catastrophic}. So how to handle the new tasks while avoiding damage to the performance of old ones becomes a major concern.

To solve the \textit{Catastrophic Forgetting} problem, there has been a substantial number of works published in recent years. Generally speaking, the current works can be divided into three categories: model adaptation \cite{aljundi2017expert, serra2018overcoming, mallya2018packnet}, memory replay \cite{rebuffi2017icarl, lopez2017gradient, shin2017continual, iscen2020memory} and regularization \cite{li2017learning, dhar2019learning, kirkpatrick2017overcoming, hou2019learning}. Model adaptation attacks the problem by specializing the parameters or adding supportive structures to accommodate new tasks. Memory replay stores and replays representative data of old tasks during training on new tasks. Regularization restricts the extent of forgetting by adding old-task-related constraints to the loss function. In addition to that, recent works \cite{hu2021distilling, douillard2020podnet} are also combining memory replay with regularization to achieve better performances

Despite the diverse angles of dealing with this problem, these models are based on the same foundation, that is using one shared feature space to describe all different tasks. But this is actually a simplified condition, since different tasks are naturally of various feature spaces. The disadvantage of this simplification shows when two tasks are largely unrelated. Under that setting, the intersection of two tasks' feature spaces will be small and hardly contains useful information for either task. This problem frequently appears in incremental learning experiments in a way that the order of tasks can affect the overall performance of the model. 

Given the consideration of various feature spaces for different tasks, the current models have difficulty being adapted to this more complex situation. Because they will train the same feature extractor, which we note as $\theta$, to find features of all tasks. The different feature spaces could only be represented by projecting themselves onto the shared feature space. But it is inevitable that task-specific knowledge would be lost during this progress. To preserve the specific knowledge, it is important to treat the various features spaces as combinations of shared and task-specific parts and process two parts separately. This leads to the requirement of modifying the structure of $\theta$.

Regarding this situation, we propose our model ILVFS(temp). Following the extraction-classification procedure, to separate shared and specific features, we split $\theta$ into shared feature extractor $\theta_s$ that focuses on extracting the shared features and specific feature extractors $\theta_{sp}$ that are responsible for capturing each tasks' specific features. The final classification would then be given based on both kinds of features. To ensure that the focus of each $\theta_{sp}$ is on task-specific information, we introduce the concept of discriminability score, which examines the specialty and effectiveness of $\theta_{sp}$ on its corresponding task compared with other tasks. 

Our contributions are, 1) we propose a novel structure of the model that can manage task increment with various feature spaces, 2) we define and utilize discriminability score to capture the task-specific knowledge. We conducted experiments based on tabular data sets, which are sensitive to the variant of feature spaces. Our model outperforms the baseline models and approaches the performance of the joint learning setting, where all tasks' data are available at each task increment.

% \section{Related Works}

% Comparing each entry of data as a sentence, different number of features corresponds to sentences of various lengths, which leads the way of using the transformer as basic structure. Because the transformer can absorb sentences of various lengths and then convert them into unified representations, we could train each task by its full features.


\bibliography{citation}
\end{document}
\endinput